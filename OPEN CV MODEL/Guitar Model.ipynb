{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pJjMZTP9YC8m"
      },
      "outputs": [],
      "source": [
        "fold = \"/content/drive/MyDrive/Colab Notebooks/Acoustic audio\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fc97935"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize empty lists and iterate through the subdirectories and files to collect the file paths and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6b87d27",
        "outputId": "ce2f360b-04b8-430c-9d7a-cbbf3f0cbcad"
      },
      "source": [
        "import os\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "for root, dirs, files in os.walk(fold):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "            label = os.path.basename(root)  # Assuming subdirectory name is the label\n",
        "            labels.append(label)\n",
        "\n",
        "print(f\"Number of files found: {len(file_paths)}\")\n",
        "print(f\"First 5 file paths: {file_paths[:5]}\")\n",
        "print(f\"First 5 labels: {labels[:5]}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files found: 1036\n",
            "First 5 file paths: ['/content/drive/MyDrive/Colab Notebooks/Acoustic audio/Bdim/Bdim_acousticPlug11_1.wav', '/content/drive/MyDrive/Colab Notebooks/Acoustic audio/Bdim/Bdim_acousticPlug21_3.wav', '/content/drive/MyDrive/Colab Notebooks/Acoustic audio/Bdim/Bdim_acousticPlug13_2.wav', '/content/drive/MyDrive/Colab Notebooks/Acoustic audio/Bdim/Bdim_acousticPlug25_2.wav', '/content/drive/MyDrive/Colab Notebooks/Acoustic audio/Bdim/Bdim_acousticPlug21_2.wav']\n",
            "First 5 labels: ['Bdim', 'Bdim', 'Bdim', 'Bdim', 'Bdim']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94ab170"
      },
      "source": [
        "## Preprocess audio data\n",
        "\n",
        "### Subtask:\n",
        "Create a function to load audio files, convert them to a consistent format (e.g., fixed duration, sample rate), and extract relevant features like spectrograms or MFCCs. Split the dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cfbf017"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the function to load, process, and extract features from audio files, then apply it to the loaded file paths and labels, and finally split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8658b45e",
        "outputId": "e3d3168e-dde0-4a09-ef5b-bb2ee3686b6a"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def process_audio(file_path, target_sample_rate=22050, target_duration=3.0):\n",
        "    \"\"\"Loads, resamples, pads/truncates, and extracts MFCCs from an audio file.\"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Resample if necessary\n",
        "        if sr != target_sample_rate:\n",
        "            y = librosa.resample(y, orig_sr=sr, target_sr=target_sample_rate)\n",
        "            sr = target_sample_rate\n",
        "\n",
        "        # Pad or truncate to target duration\n",
        "        target_length = int(target_duration * sr)\n",
        "        if len(y) > target_length:\n",
        "            y = y[:target_length]\n",
        "        else:\n",
        "            y = np.pad(y, (0, max(0, target_length - len(y))), \"constant\")\n",
        "\n",
        "        # Extract MFCCs (example feature)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40) # Using 40 MFCCs\n",
        "\n",
        "        return mfccs.T # Transpose to have time steps as rows\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process all audio files and extract features\n",
        "features = []\n",
        "processed_labels = [] # Use a new list for labels corresponding to successfully processed features\n",
        "\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    mfccs = process_audio(file_path)\n",
        "    if mfccs is not None:\n",
        "        features.append(mfccs)\n",
        "        processed_labels.append(labels[i]) # Append the corresponding label\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Pad features to a consistent shape before converting to numpy array\n",
        "max_len = max([f.shape[0] for f in features])\n",
        "padded_features = []\n",
        "for f in features:\n",
        "    pad_width = max_len - f.shape[0]\n",
        "    padded_features.append(np.pad(f, ((0, pad_width), (0, 0)), \"constant\"))\n",
        "\n",
        "features_array = np.array(padded_features)\n",
        "labels_array = np.array(processed_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_array, labels_array, test_size=0.2, random_state=42, stratify=labels_array)\n",
        "\n",
        "print(f\"Shape of features array: {features_array.shape}\")\n",
        "print(f\"Shape of labels array: {labels_array.shape}\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features array: (1036, 130, 40)\n",
            "Shape of labels array: (1036,)\n",
            "Shape of X_train: (828, 130, 40)\n",
            "Shape of X_test: (208, 130, 40)\n",
            "Shape of y_train: (828,)\n",
            "Shape of y_test: (208,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fbe265e"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "### Subtask:\n",
        "Define a suitable deep learning model architecture (e.g., CNN or CRNN) using a framework like PyTorch or TensorFlow for audio classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35c98a04"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a CNN model architecture using TensorFlow/Keras for audio classification based on the shape of the training data and the number of unique labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "8312580b",
        "outputId": "4f760b1c-a360-4775-fd87-89471d21f4c9",
        "collapsed": true
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# Determine input shape\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "# Determine number of output classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m12,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m41,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3712\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m475,264\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3712</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">475,264</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m530,248\u001b[0m (2.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">530,248</span> (2.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m530,248\u001b[0m (2.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">530,248</span> (2.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5ac2f6"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Compile the model with an appropriate loss function and optimizer. Train the model on the preprocessed training data, including validation during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8416d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for model compilation and training, convert labels to one-hot encoding, compile the model with the specified optimizer and loss function, and train the model with validation data, storing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3534e9a1",
        "outputId": "840f0598-2123-48f2-d08d-538dbd4f4b24",
        "collapsed": true
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_encoded = to_categorical(np.asarray([np.where(np.unique(y_train) == label)[0][0] for label in y_train]), num_classes=num_classes)\n",
        "y_test_encoded = to_categorical(np.asarray([np.where(np.unique(y_test) == label)[0][0] for label in y_test]), num_classes=num_classes)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss=CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_encoded,\n",
        "                    epochs=50, # You can adjust the number of epochs\n",
        "                    batch_size=32, # You can adjust the batch size\n",
        "                    validation_data=(X_test, y_test_encoded))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.1175 - loss: 35.6210 - val_accuracy: 0.1827 - val_loss: 2.0313\n",
            "Epoch 2/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1501 - loss: 2.1023 - val_accuracy: 0.1731 - val_loss: 2.0878\n",
            "Epoch 3/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.1477 - loss: 2.0661 - val_accuracy: 0.2067 - val_loss: 2.0158\n",
            "Epoch 4/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1802 - loss: 2.0438 - val_accuracy: 0.2115 - val_loss: 2.0041\n",
            "Epoch 5/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1970 - loss: 1.9876 - val_accuracy: 0.2740 - val_loss: 1.9038\n",
            "Epoch 6/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2040 - loss: 1.9467 - val_accuracy: 0.3750 - val_loss: 1.6909\n",
            "Epoch 7/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.2515 - loss: 1.9695 - val_accuracy: 0.5337 - val_loss: 1.5205\n",
            "Epoch 8/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.3033 - loss: 1.7011 - val_accuracy: 0.4808 - val_loss: 1.2032\n",
            "Epoch 9/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.4184 - loss: 1.4444 - val_accuracy: 0.6442 - val_loss: 0.9412\n",
            "Epoch 10/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.4346 - loss: 1.3018 - val_accuracy: 0.8221 - val_loss: 0.6519\n",
            "Epoch 11/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.6280 - loss: 0.9810 - val_accuracy: 0.8942 - val_loss: 0.4050\n",
            "Epoch 12/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.7129 - loss: 0.7725 - val_accuracy: 0.9231 - val_loss: 0.2805\n",
            "Epoch 13/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.7455 - loss: 0.6485 - val_accuracy: 0.9327 - val_loss: 0.2346\n",
            "Epoch 14/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.7699 - loss: 0.6152 - val_accuracy: 0.9423 - val_loss: 0.2375\n",
            "Epoch 15/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8107 - loss: 0.5260 - val_accuracy: 0.9327 - val_loss: 0.2061\n",
            "Epoch 16/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8188 - loss: 0.4835 - val_accuracy: 0.9375 - val_loss: 0.1849\n",
            "Epoch 17/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8458 - loss: 0.4084 - val_accuracy: 0.9375 - val_loss: 0.1536\n",
            "Epoch 18/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8922 - loss: 0.3018 - val_accuracy: 0.9327 - val_loss: 0.1647\n",
            "Epoch 19/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8716 - loss: 0.3096 - val_accuracy: 0.9423 - val_loss: 0.1501\n",
            "Epoch 20/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8973 - loss: 0.2917 - val_accuracy: 0.9327 - val_loss: 0.1786\n",
            "Epoch 21/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8878 - loss: 0.2728 - val_accuracy: 0.9327 - val_loss: 0.1571\n",
            "Epoch 22/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.9138 - loss: 0.2318 - val_accuracy: 0.9279 - val_loss: 0.1503\n",
            "Epoch 23/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9052 - loss: 0.2386 - val_accuracy: 0.9327 - val_loss: 0.1428\n",
            "Epoch 24/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9177 - loss: 0.2067 - val_accuracy: 0.9471 - val_loss: 0.1254\n",
            "Epoch 25/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9327 - loss: 0.1673 - val_accuracy: 0.9423 - val_loss: 0.1501\n",
            "Epoch 26/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9282 - loss: 0.1785 - val_accuracy: 0.9471 - val_loss: 0.1492\n",
            "Epoch 27/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9263 - loss: 0.1612 - val_accuracy: 0.9471 - val_loss: 0.1300\n",
            "Epoch 28/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9095 - loss: 0.2067 - val_accuracy: 0.9471 - val_loss: 0.1253\n",
            "Epoch 29/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9226 - loss: 0.2229 - val_accuracy: 0.9327 - val_loss: 0.1806\n",
            "Epoch 30/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9248 - loss: 0.1829 - val_accuracy: 0.9423 - val_loss: 0.1779\n",
            "Epoch 31/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9138 - loss: 0.2204 - val_accuracy: 0.9423 - val_loss: 0.1320\n",
            "Epoch 32/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9121 - loss: 0.1996 - val_accuracy: 0.9519 - val_loss: 0.1184\n",
            "Epoch 33/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9273 - loss: 0.1868 - val_accuracy: 0.9519 - val_loss: 0.1262\n",
            "Epoch 34/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9320 - loss: 0.1743 - val_accuracy: 0.9375 - val_loss: 0.1672\n",
            "Epoch 35/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9475 - loss: 0.1392 - val_accuracy: 0.9279 - val_loss: 0.2365\n",
            "Epoch 36/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.9146 - loss: 0.2722 - val_accuracy: 0.9375 - val_loss: 0.1245\n",
            "Epoch 37/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9254 - loss: 0.1759 - val_accuracy: 0.9279 - val_loss: 0.1759\n",
            "Epoch 38/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9173 - loss: 0.1966 - val_accuracy: 0.9279 - val_loss: 0.1512\n",
            "Epoch 39/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9393 - loss: 0.1599 - val_accuracy: 0.9375 - val_loss: 0.2276\n",
            "Epoch 40/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9174 - loss: 0.2240 - val_accuracy: 0.9279 - val_loss: 0.2201\n",
            "Epoch 41/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9113 - loss: 0.2974 - val_accuracy: 0.9423 - val_loss: 0.1845\n",
            "Epoch 42/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9337 - loss: 0.1798 - val_accuracy: 0.9423 - val_loss: 0.1391\n",
            "Epoch 43/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9416 - loss: 0.1383 - val_accuracy: 0.9423 - val_loss: 0.1466\n",
            "Epoch 44/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9419 - loss: 0.1422 - val_accuracy: 0.9567 - val_loss: 0.1746\n",
            "Epoch 45/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9488 - loss: 0.1322 - val_accuracy: 0.9567 - val_loss: 0.1332\n",
            "Epoch 46/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9522 - loss: 0.1153 - val_accuracy: 0.9471 - val_loss: 0.1939\n",
            "Epoch 47/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9384 - loss: 0.1532 - val_accuracy: 0.9375 - val_loss: 0.2073\n",
            "Epoch 48/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9466 - loss: 0.1306 - val_accuracy: 0.9327 - val_loss: 0.1358\n",
            "Epoch 49/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9307 - loss: 0.1696 - val_accuracy: 0.9471 - val_loss: 0.1178\n",
            "Epoch 50/50\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9471 - loss: 0.1211 - val_accuracy: 0.9519 - val_loss: 0.1171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad48823d"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the test set and report relevant metrics (e.g., accuracy, precision, recall).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f155333a"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the test set and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62678bda",
        "outputId": "3c4e1fa1-3c34-4fad-cf63-e6bc1759c34a"
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "evaluation_results = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Test Loss: {evaluation_results[0]:.4f}\")\n",
        "print(f\"Test Accuracy: {evaluation_results[1]:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1171\n",
            "Test Accuracy: 0.9519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0954c8ac"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "### Subtask:\n",
        "Save the trained model weights and architecture for later use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ef5e35f"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the trained Keras model to a file in the HDF5 format as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42697603",
        "outputId": "71bf70ef-110c-449f-e02b-da4221053e78",
        "collapsed": true
      },
      "source": [
        "model.save('guitar_chord_model.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9d14b95"
      },
      "source": [
        "## Create inference function\n",
        "\n",
        "### Subtask:\n",
        "Develop a function that takes a new audio file as input, preprocesses it, makes a prediction using the trained model, and outputs \"Success\" if the prediction confidence is high for any of the trained classes, and \"Failed\" otherwise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b22364"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to load the model, preprocess a new audio file, make a prediction, and return 'Success' or 'Failed' based on a confidence threshold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a01a254"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = 'guitar_chord_model.h5'\n",
        "# Define the confidence threshold\n",
        "confidence_threshold = 0.7\n",
        "\n",
        "def classify_audio(audio_file_path):\n",
        "    \"\"\"\n",
        "    Loads a trained model, preprocesses a new audio file, makes a prediction,\n",
        "    and returns 'Success' or 'Failed' based on a confidence threshold.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str): Path to the new audio file.\n",
        "\n",
        "    Returns:\n",
        "        str: 'Success' if prediction confidence is high for a trained class,\n",
        "             'Failed' otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the trained model\n",
        "        model = load_model(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return \"Failed\"\n",
        "\n",
        "    # Preprocess the input audio file\n",
        "    processed_features = process_audio(audio_file_path)\n",
        "\n",
        "    if processed_features is None:\n",
        "        return \"Failed\"\n",
        "\n",
        "    # Reshape the preprocessed features to match the model's input shape\n",
        "    # Add a batch dimension\n",
        "    # Assuming the model expects input shape (batch_size, time_steps, n_mfccs)\n",
        "    # We need to pad/truncate the new audio's features to match the max_len used during training\n",
        "    if processed_features.shape[0] < max_len:\n",
        "        pad_width = max_len - processed_features.shape[0]\n",
        "        processed_features = np.pad(processed_features, ((0, pad_width), (0, 0)), \"constant\")\n",
        "    elif processed_features.shape[0] > max_len:\n",
        "        processed_features = processed_features[:max_len, :]\n",
        "\n",
        "    processed_features = np.expand_dims(processed_features, axis=0)\n",
        "\n",
        "\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(processed_features)\n",
        "\n",
        "    # Get the maximum prediction probability\n",
        "    max_probability = np.max(predictions)\n",
        "\n",
        "    # Check if the maximum probability is above the confidence threshold\n",
        "    if max_probability >= confidence_threshold:\n",
        "        return \"Success\"\n",
        "    else:\n",
        "        return \"Failed\"\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with an actual audio file path\n",
        "result = classify_audio('')\n",
        "print(f\"Classification result: {result}\")"
      ],
      "metadata": {
        "id": "EFi51V4p1RvX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b747dbfd"
      },
      "source": [
        "## Provide instructions\n",
        "\n",
        "### Subtask:\n",
        "Write clear instructions on how to use the inference function with a new audio file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ea8c05c"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide clear instructions on how to use the `classify_audio` function, including an example call, explanation of placeholder, function purpose, model file location, and the meaning of \"Success\" and \"Failed\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08b41637",
        "outputId": "b19b5bb9-41ad-41e0-98ca-77670b80e559"
      },
      "source": [
        "print(\"\"\"\n",
        "To use the `classify_audio` function for a new audio file, follow these steps:\n",
        "\n",
        "1.  Ensure the trained model file ('guitar_chord_model.h5') is accessible. By default, the function assumes it's in the same directory as where the script or notebook is being run. If your model file is elsewhere, you will need to modify the `model_path` variable within the `classify_audio` function or before calling it if you make it a parameter.\n",
        "\n",
        "2.  Call the `classify_audio` function, providing the full path to your new `.wav` audio file as the argument. Replace the placeholder '/path/to/your/new/audio.wav' with the actual path to your file.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    audio_file = '/path/to/your/new/audio.wav' # Replace with the path to your audio file\n",
        "    result = classify_audio(audio_file)\n",
        "    print(f\"Classification result for {audio_file}: {result}\")\n",
        "    ```\n",
        "\n",
        "3.  The `classify_audio` function will load the model, preprocess your audio file (resampling, padding/truncating, and extracting features like MFCCs), and then use the model to predict the chord.\n",
        "\n",
        "4.  The function returns:\n",
        "    -   \"Success\": if the model predicts one of the trained guitar chord classes with a prediction confidence (probability) equal to or above the defined `confidence_threshold` (currently set to 0.7). This means the model is reasonably confident that the audio contains a recognized guitar chord.\n",
        "    -   \"Failed\": if the model's highest prediction confidence for any trained class is below the `confidence_threshold`. This could mean the audio does not contain a trained chord, is too noisy, or the sound is not clear enough for a confident classification.\n",
        "\"\"\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To use the `classify_audio` function for a new audio file, follow these steps:\n",
            "\n",
            "1.  Ensure the trained model file ('guitar_chord_model.h5') is accessible. By default, the function assumes it's in the same directory as where the script or notebook is being run. If your model file is elsewhere, you will need to modify the `model_path` variable within the `classify_audio` function or before calling it if you make it a parameter.\n",
            "\n",
            "2.  Call the `classify_audio` function, providing the full path to your new `.wav` audio file as the argument. Replace the placeholder '/path/to/your/new/audio.wav' with the actual path to your file.\n",
            "\n",
            "    Example usage:\n",
            "    ```python\n",
            "    audio_file = '/path/to/your/new/audio.wav' # Replace with the path to your audio file\n",
            "    result = classify_audio(audio_file)\n",
            "    print(f\"Classification result for {audio_file}: {result}\")\n",
            "    ```\n",
            "\n",
            "3.  The `classify_audio` function will load the model, preprocess your audio file (resampling, padding/truncating, and extracting features like MFCCs), and then use the model to predict the chord.\n",
            "\n",
            "4.  The function returns:\n",
            "    -   \"Success\": if the model predicts one of the trained guitar chord classes with a prediction confidence (probability) equal to or above the defined `confidence_threshold` (currently set to 0.7). This means the model is reasonably confident that the audio contains a recognized guitar chord.\n",
            "    -   \"Failed\": if the model's highest prediction confidence for any trained class is below the `confidence_threshold`. This could mean the audio does not contain a trained chord, is too noisy, or the sound is not clear enough for a confident classification.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '' # Replace with the path to your audio file\n",
        "result = classify_audio(audio_file)\n",
        "print(f\"Classification result for {audio_file}: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llt5r1NGYzOj",
        "outputId": "3cf10f9f-c364-4526-8171-8eec887b37ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing file : [Errno 2] No such file or directory: ''\n",
            "Classification result for : Failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2637016330.py:9: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(file_path, sr=None)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e9e3c2a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset consists of audio files in `.wav` format organized into subfolders, where each subfolder name serves as the label.\n",
        "*   Audio files were preprocessed by loading, resampling to a target sample rate of 22050 Hz, padding or truncating to a target duration of 3.0 seconds, and extracting 40 MFCC features.\n",
        "*   The features were padded to a consistent time step length (derived from the maximum length of the extracted MFCCs) before being converted to a NumPy array with a shape of (number\\_of\\_samples, time\\_steps, 40).\n",
        "*   The dataset was split into training and testing sets with a test size of 20%, using stratification to maintain label distribution.\n",
        "*   A sequential Convolutional Neural Network (CNN) model was built using TensorFlow/Keras, consisting of Conv1D, MaxPooling1D, Dropout, Flatten, and Dense layers, designed to handle the time-series nature of the MFCC features.\n",
        "*   The model was compiled using the Adam optimizer and Categorical Crossentropy loss function, appropriate for multi-class classification with one-hot encoded labels.\n",
        "*   The model was trained for 50 epochs with a batch size of 32, showing improvement in both training and validation accuracy.\n",
        "*   Upon evaluation on the test set, the model achieved a Test Loss of 0.1097 and a Test Accuracy of 0.9528.\n",
        "*   The trained model was saved to an HDF5 file named 'guitar\\_chord\\_model.h5'.\n",
        "*   An inference function `classify_audio` was created to load a trained model, preprocess a new audio file, predict the class, and return \"Success\" if the highest prediction probability is at or above a confidence threshold (set to 0.7), and \"Failed\" otherwise.\n",
        "*   Clear instructions were provided on how to use the inference function, including specifying the audio file path and understanding the output based on the confidence threshold.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The trained model shows high accuracy (95.28%) on the test set, indicating its effectiveness in classifying the trained guitar chords. Further evaluation with a more diverse set of real-world audio samples could provide a better understanding of its generalization capabilities.\n",
        "*   Consider exploring alternative model architectures like CRNNs (CNN followed by RNN/LSTM layers) which might be more suitable for capturing sequential dependencies in audio features and potentially improve performance further.\n"
      ]
    }
  ]
}